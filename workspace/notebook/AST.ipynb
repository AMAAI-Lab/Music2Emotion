{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'is_floating_point'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m resample_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16000\u001b[39m\n\u001b[1;32m     18\u001b[0m resampler \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mResample(sampling_rate, resample_rate)\n\u001b[0;32m---> 19\u001b[0m waveform \u001b[38;5;241m=\u001b[39m \u001b[43mresampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# waveform = waveform.squeeze().numpy()\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(waveform\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/micromamba/envs/music2emo/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/music2emo/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/music2emo/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:979\u001b[0m, in \u001b[0;36mResample.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_freq:\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m waveform\n\u001b[0;32m--> 979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_sinc_resample_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/music2emo/lib/python3.10/site-packages/torchaudio/functional/functional.py:1452\u001b[0m, in \u001b[0;36m_apply_sinc_resample_kernel\u001b[0;34m(waveform, orig_freq, new_freq, gcd, kernel, width)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_sinc_resample_kernel\u001b[39m(\n\u001b[1;32m   1445\u001b[0m     waveform: Tensor,\n\u001b[1;32m   1446\u001b[0m     orig_freq: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1450\u001b[0m     width: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   1451\u001b[0m ):\n\u001b[0;32m-> 1452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwaveform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m():\n\u001b[1;32m   1453\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected floating point type for waveform tensor, but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwaveform\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1455\u001b[0m     orig_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(orig_freq) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m gcd\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'is_floating_point'"
     ]
    }
   ],
   "source": [
    "from transformers import ASTFeatureExtractor\n",
    "\n",
    "feature_extractor = ASTFeatureExtractor()\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "\n",
    "\n",
    "waveform, sampling_rate = torchaudio.load(\"../data/7400.mp3\")\n",
    "\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = waveform.mean(dim=0).unsqueeze(0)\n",
    "waveform = waveform.squeeze().numpy()\n",
    "\n",
    "resample_rate = 16000\n",
    "\n",
    "resampler = T.Resample(sampling_rate, resample_rate)\n",
    "waveform = resampler(waveform)\n",
    "\n",
    "# waveform = waveform.squeeze().numpy()\n",
    "\n",
    "\n",
    "print(waveform.shape)\n",
    "\n",
    "\n",
    "inputs = feature_extractor(waveform, sampling_rate=sampling_rate, padding=\"max_length\", return_tensors=\"pt\")\n",
    "input_values = inputs.input_values\n",
    "print(input_values.shape)\n",
    "\n",
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2336000])\n",
      "torch.Size([1, 1024, 128])\n",
      "Predicted class: Music\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "from transformers import ASTFeatureExtractor, AutoModelForAudioClassification\n",
    "\n",
    "# Load the waveform and its sample rate\n",
    "waveform, sampling_rate = torchaudio.load(\"../data/7400.mp3\")\n",
    "\n",
    "# If the waveform has more than one channel, average the channels\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = waveform.mean(dim=0).unsqueeze(0)\n",
    "\n",
    "# Resample the waveform if necessary\n",
    "resample_rate = 16000\n",
    "resampler = T.Resample(sampling_rate, resample_rate)\n",
    "waveform = resampler(waveform)\n",
    "\n",
    "# No need to convert to numpy, keep as a tensor\n",
    "print(waveform.shape)\n",
    "\n",
    "# Load feature extractor and extract features\n",
    "feature_extractor = ASTFeatureExtractor()\n",
    "inputs = feature_extractor(waveform.squeeze().numpy(), sampling_rate=resample_rate, padding=\"max_length\", return_tensors=\"pt\")\n",
    "input_values = inputs.input_values\n",
    "print(input_values.shape)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(input_values)\n",
    "\n",
    "predicted_class_idx = outputs.logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 527])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTForAudioClassification(\n",
      "  (audio_spectrogram_transformer): ASTModel(\n",
      "    (embeddings): ASTEmbeddings(\n",
      "      (patch_embeddings): ASTPatchEmbeddings(\n",
      "        (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ASTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ASTLayer(\n",
      "          (attention): ASTSdpaAttention(\n",
      "            (attention): ASTSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): ASTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ASTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ASTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): ASTMLPHead(\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dense): Linear(in_features=768, out_features=527, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import ASTFeatureExtractor\n",
    "\n",
    "feature_extractor = ASTFeatureExtractor()\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Music\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6438600)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "waveform, sampling_rate = torchaudio.load(\"../data/7400.mp3\")\n",
    "waveform = waveform.squeeze().numpy()\n",
    "\n",
    "waveform.shape\n",
    "\n",
    "# if waveform.shape[0] > 1:\n",
    "#     waveform = waveform.mean(dim=0).unsqueeze(0)\n",
    "# waveform = waveform.squeeze()\n",
    "\n",
    "# resample_rate = processor.sampling_rate\n",
    "# if sample_rate != resample_rate:\n",
    "#     print(f\"Resampling from {sample_rate} to {resample_rate}\")\n",
    "#     resampler = T.Resample(sample_rate, resample_rate)\n",
    "#     waveform = resampler(waveform)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music2emo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
